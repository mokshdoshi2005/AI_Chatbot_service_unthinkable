# AI_Chatbot_service_unthinkable
Objective:  Simulate customer support interactions using AI for FAQs and escalation scenarios. For: Unthinkable Solutions
> Intelligent FAQ handling with contextual memory & smart escalation A production-ready prototype demonstrating AI-powered customer support with conversation context retention, automatic escalation logic, and session management.
## ğŸ¯ Core Features 
- **Contextual Conversations**
- - Maintains conversation history for coherent multi-turn interactions
- **Intelligent FAQ Matching**
- - LLM-powered response generation from knowledge base
- **Smart Escalation**
- - Automatically escalates unresolved queries to human support
- **Session Management**
- - Persistent conversation tracking across user sessions
- **RESTful API**
- - Clean, documented endpoints for easy integration

## ğŸ—ï¸ Architecture 
``` â”œâ”€â”€ backend/ â”‚ â”œâ”€â”€ api/ # REST endpoint handlers â”‚ â”œâ”€â”€ services/ # LLM integration & business logic â”‚ â”œâ”€â”€ models/ # Database schemas â”‚ â””â”€â”€ utils/ # Helper functions â”œâ”€â”€ data/ â”‚ â””â”€â”€ faqs.json # Knowledge base â””â”€â”€ frontend/ # Optional chat interface ``` 
## ğŸš€ Quick Start 
### Prerequisites 
```bash Python 3.9+ OpenAI API Key / Anthropic API Key ``` 
### Installation 
```bash # Clone repository git clone <repo-url> cd ai-customer-support-bot # Install dependencies pip install -r requirements.txt # Configure environment cp .env.example .env # Add your API keys to .env ``` 
### Run 
```bash # Start backend server python app.py # Server runs on http://localhost:5000 ``` 
## ğŸ“¡ API Endpoints 
### `POST /api/chat` 
Send a customer query and receive AI-generated response. 
**Request:** ```json { "session_id": "user-123", "message": "How do I reset my password?", "user_id": "customer-456" } ``` 
**Response:** ```json { "response": "To reset your password, click on...", "escalated": false, "session_id": "user-123", "confidence": 0.92 } ``` 
### `GET /api/session/<session_id>` 
Retrieve conversation history for a session.
### `POST /api/escalate` 
Manually escalate a conversation to human support. 

## ğŸ§  LLM Integration 
### Prompt Engineering Strategy 
**1. Response Generation** ``` System: You are a helpful customer support agent with access to FAQs. Context: [Previous conversation history] Knowledge: [Relevant FAQ entries] Query: [Customer question] Task: Provide accurate, friendly response. Say "I'll escalate this" if uncertain. ``` 
**2. Conversation Summarization** - Summarizes long conversations for context retention - Extracts key intents and unresolved issues 
**3. Escalation Decision** ``` Analyze if query requires human support based on: - Confidence score < 0.7 - Complex/sensitive issues (billing, legal, complaints) - Multiple failed resolution attempts ``` 
## ğŸ’¾ Session Management 
- **In-Memory Storage** (Prototype) - Redis/Database ready
- **Session TTL** - 30 minutes of inactivity
- **Context Window** - Last 10 messages retained for LLM context
- **Metadata Tracking** - Timestamps, user info, escalation status

## ğŸ¨ Frontend (Optional) 
Simple chat interface built with vanilla JS + Tailwind CSS 
- Real-time message rendering
- Session persistence
- Escalation indicators
- Responsive design Access at: `http://localhost:5000/`
## ğŸ”§ Configuration Key environment variables in `.env`: 
```bash # LLM Provider LLM_PROVIDER=openai # openai | anthropic LLM_API_KEY=your-key-here LLM_MODEL=gpt-4o-mini # or claude-3-5-sonnet-20241022 # App Settings SESSION_TIMEOUT=1800 # seconds MAX_CONTEXT_MESSAGES=10 ESCALATION_THRESHOLD=0.7 ``` 
## ğŸ“Š Evaluation Metrics 
- **Response Accuracy** - Measured against FAQ ground truth
- **Context Retention** - Handles follow-up questions correctly
- **Escalation Logic** - Appropriately routes complex queries
- **API Performance** - <500ms average response time

## ğŸ§ª Testing 
```bash # Run test suite pytest tests/ # Test individual endpoint curl -X POST http://localhost:5000/api/chat \ -H "Content-Type: application/json" \ -d '{"session_id":"test-1","message":"Hello"}' ``` 

## ğŸ“ Technical Decisions 
### Why These Choices? 
**LLM Provider Agnostic** - Abstracted LLM interface supports OpenAI/Anthropic/local models - Easy to swap providers without code changes 
**Simple Session Store** - In-memory dict for prototype speed - Production: Redis with same interface 
**Minimal Dependencies** - Flask (lightweight, familiar) - No heavy frameworks (given time constraint) - Can migrate to FastAPI for async if needed 
**Conversation Context Strategy** - Sliding window (last N messages) balances context quality vs. token cost - Summaries for very long conversations 
## ğŸš§ Known Limitations - In-memory sessions (not persistent across restarts) - Basic error handling (production needs retry logic, rate limiting) - No authentication layer - FAQ similarity search is LLM-based (embedding search would be faster) 
## ğŸ”® Production Roadmap 
- [ ] Database persistence (PostgreSQL + Redis)
- [ ] Vector search for FAQ retrieval (Pinecone/Weaviate)
- [ ] User authentication & rate limiting
- [ ] Monitoring & logging (DataDog/Sentry)
- [ ] A/B testing framework for prompts
- [ ] Multi-language support

## ğŸ“š Key Dependencies 
``` flask>=3.0.0 openai>=1.0.0 anthropic>=0.21.0 python-dotenv>=1.0.0 ``` 
## Contributing 
This is a prototype. For production use: 1. Add comprehensive tests 2. Implement proper error handling 3. Add authentication middleware 4. Set up CI/CD pipeline
